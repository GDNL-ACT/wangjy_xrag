# xRAG
## è¿è¡Œè¯´æ˜
1. åˆ›å»ºcondaç¯å¢ƒï¼š
```
conda create -n xrag python=3.9 -y
conda activate xrag
```

2. å®‰è£…ä¾èµ–ï¼š
```
pip install -r requirements.txt
```
ç”±äºflash-attnæ„å»ºè¾ƒæ…¢ï¼Œä½¿ç”¨é¢„ä¸‹è½½çš„whlåŒ…å®‰è£…ï¼Œå¦‚æœ‰ç‰ˆæœ¬é—®é¢˜å¯ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ï¼šï¼ˆhttps://github.com/Dao-AILab/flash-attention/releases?expanded=true&page=5&q=ï¼‰

3. é…ç½®ç¯å¢ƒå˜é‡ï¼š
```
export HF_ENDPOINT=https://hf-mirror.com
export HF_HOME=$PWD/huggingface  # huggingfaceå­˜å‚¨ä½ç½®
export WANDB_MODE=offline
export WANDB_DIR=$PWD/wandb  # wandbå­˜å‚¨ä½ç½®
```

4. ä¸‹è½½é¢„è®­ç»ƒçš„wikiæ•°æ®é›†å¹¶åˆ†å‰²ï¼Œä¸‹è½½ç»“æœå­˜å‚¨åœ¨data/pretrain/wikipediaä¸­ï¼›ä¸‹è½½æŒ‡ä»¤å¾®è°ƒçš„æ•°æ®é›†ï¼Œä¸‹è½½å†…å®¹å­˜å‚¨åœ¨huggingfaceç¼“å­˜ä¸­ï¼Œæœ€ç»ˆæ•°æ®é›†å­˜å‚¨åœ¨data/instruction_tuning/processed/context_aware_instrution_tuning_data.jsonlï¼š
```
bash prepare_data.sh
```

5. é¢„è®­ç»ƒï¼Œå‚æ•°ä¸­åŒ…å«ä½¿ç”¨çš„å¡æ•°ã€batch sizeã€æ¢¯åº¦ç§¯ç´¯ç­‰ï¼š
```
bash pretrain.sh
```
è®­ç»ƒæ—¥å¿—å’Œcheckpointsç”±wandbä¿å­˜åœ¨ç›¸å…³ç›®å½•ä¸­ï¼Œtokenizeç»“æœä¼šç¼“å­˜åœ¨huggingfaceç›®å½•ä¸­ã€‚

6. æŒ‡ä»¤å¾®è°ƒï¼Œé»˜è®¤åŠ è½½wandbè®°å½•çš„æœ€æ–°è®­ç»ƒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯æ‰‹åŠ¨æ›´æ”¹model_name_or_pathæŒ‡å®šæ¨¡å‹
```
bash instruction_tuning.sh
```

7. æ¨¡å‹è¯„ä¼°ï¼Œå¤šå¡è·‘å¤šä¸ªæµ‹è¯•ä»»åŠ¡ï¼Œç»“æœå­˜åœ¨eval_res/SFR-Embedding-Mistralä¸‹ï¼Œä¸è¿‡ç›®å‰æ•°æ®é›†è¿˜éœ€è¦é‡æ–°é€‰æ‹©ï¼š
```
bash eval_all.sh
```


ä»¥ä¸‹ä¸ºåŸå§‹readme
## Introduction
Official repo for [xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token](https://arxiv.org/abs/2405.13792)

## Get Started
Refer to `Dockerfile` for required packages

Configure `wandb` and `accelerate`
```bash
wandb login
accelerate config
```

## Pretrained Checkpoints
HuggingFace
| Model                 | Backbone | Download                                                                    |
|-----------------------|-----------------|-----------------------------------------------------------------------------|
| xRAG-7b | [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)            | [ğŸ¤— Hugging Face](https://huggingface.co/Hannibal046/xrag-7b) |
| xRAG-MoE | [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)            | [ğŸ¤— Hugging Face](https://huggingface.co/Hannibal046/xrag-moe) |


## Tutorial

We provide a tutorial for xRAG in `tutorial.ipynb`. Check it out!

## Data
- download [enwiki-dec2021](https://github.com/facebookresearch/atlas?tab=readme-ov-file#models) as pretraining data and corpus for retrieval
- prepare instruction tuning data in `prepare_data.ipynb`
- download [TriviaQA](https://drive.google.com/drive/folders/1lFFTklW_0HuR53hLpFdLClgfSAhXn_2f)
- using [ColBERT-v2](https://github.com/stanford-futuredata/ColBERT.git) to conduct retrieval

## Training
Training scripts in `scripts/`, for example, to train a Mistral-7b with SFR:
```bash
accelerate launch \
    --mixed_precision bf16 \
    --num_machines 1 \
    --num_processes 8 \
    --main_process_port 29666 \
    -m \
    src.language_modeling.train \
    --config config/language_modeling/pretrain.yaml \
```
## Evaluation
The evaluation code is in `src/eval`. For example, to evaluate on TriviaQA:

without retrieval augmentation:
```bash
CUDA_VISIBLE_DEVICES=0 python -m src.eval.run_eval \
        --data triviaqa \
        --model_name_or_path Hannibal046/xrag-7b
```

with retrieval augmentation:
```bash
CUDA_VISIBLE_DEVICES=0 python -m src.eval.run_eval \
        --data triviaqa \
        --model_name_or_path Hannibal046/xrag-7b \
        --use_rag
```

with xRAG:
```bash
CUDA_VISIBLE_DEVICES=0 python -m src.eval.run_eval \
        --data triviaqa \
        --model_name_or_path Hannibal046/xrag-7b \
        --retriever_name_or_path /data1/models/BAAI/bge-m3 \
        --use_rag
```

## Benchmark
To benchmark xRAG, we provide the code in `src/language_modeling/profiler.py`.
```
python -m src.language_modeling.profiler --instruction_length 54 --generation_length 30 --dataset triviaqa --use_xrag
python -m src.language_modeling.profiler --instruction_length 54 --generation_length 30 --dataset triviaqa
```
